{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a3b267-5088-47db-9fe5-0d5e5b4cfe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d10f5e5-40fa-4b48-973f-9e5c2aa9cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Standard normalization for CIFAR-10\n",
    "NORM_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "NORM_STD = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "# Define transforms: Resize images to 224x224 to fit standard ResNet input\n",
    "# ResNet was designed for 224x224 ImageNet images, so we resize the small CIFAR images.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(NORM_MEAN, NORM_STD)\n",
    "])\n",
    "\n",
    "# Load and Download CIFAR-10 (downloads automatically if not found)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Training Set\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "# Test/Validation Set\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b33070c-6aa4-47e6-a466-86f24ca14ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "model = resnet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4711233-1e30-45c1-8f94-ffba9608dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the data loader\n",
    "    for inputs, targets in tqdm(dataloader):\n",
    "        # Move data to the specified device (e.g., CUDA or CPU)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # 1. Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass: compute predicted outputs\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 3. Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(loss)\n",
    "        \n",
    "        # 4. Backward pass: compute gradient of the loss w.r.t model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    return loss.item() # Return the last batch loss\n",
    "\n",
    "# --- Main loop setup ---\n",
    "# Example setup (you must replace these with actual ImageNet components)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device) \n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.AdamW(\n",
    "#     model.parameters(), \n",
    "#     lr=1e-3,\n",
    "#     betas=(0.9, 0.999), \n",
    "#     eps=1e-8,\n",
    "#     weight_decay=1e-4 \n",
    "# )\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    avg_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1437c-d73c-4586-998f-e4f48c200119",
   "metadata": {},
   "source": [
    "### Evolutionary updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e6de577-db1d-47d5-b4f8-dcf54d3d554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for a bit on subsampled dataset\n",
    "import numpy as np\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "indices = np.random.choice(10_000, 10_000, replace=False)\n",
    "split1_batches = 10\n",
    "split1_dl = DataLoader(\n",
    "    [train_dataset[i] for i in indices[:BATCH_SIZE * split1_batches]],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2,\n",
    ")\n",
    "split2_dl = DataLoader(\n",
    "    [train_dataset[i] for i in indices[BATCH_SIZE * split1_batches:]],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92012fa-f359-466e-82aa-cf97950c464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "val_dataset = datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "70343a96-4f2d-4ff9-80e0-b0901f7d3544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18().to(device=device)  # reinitialize\n",
    "_ = model.eval()\n",
    "model.load_state_dict(torch.load('tmp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff0492e0-6cd3-46db-aeb7-eaa5f3ca67b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tmp.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba709511-69ca-468a-9947-c5ce475e8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467060c983e44d2c8cf29078f06669fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0373, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0372564792633057"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train for a bit on subsampled dataset with standard grad descent\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999), \n",
    "    eps=1e-8,\n",
    "    weight_decay=1e-4 \n",
    ")\n",
    "train_one_epoch(model, split1_dl, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a64b07cc-d4f7-4e78-8123-1bd63b9ee114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvMutation:\n",
    "    param_seeds: list[int] = field(default_factory=list)\n",
    "    is_identity: bool = False\n",
    "    reward: float | None = None\n",
    "    \n",
    "\n",
    "class EvOptimizer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: Iterable[nn.parameter.Parameter],\n",
    "        step_size=1e-5,\n",
    "        lr=1e-3,\n",
    "        n_mutations=64,\n",
    "        select_max=False,\n",
    "        allow_skip_mutation=True,\n",
    "        persist_parent=True,\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.mutations: list[EvMutation] = []\n",
    "        self.active_mutation: EvMutation | None = None\n",
    "        self.n_mutations = n_mutations\n",
    "        self.step_size = step_size\n",
    "        self.lr = lr\n",
    "        self.select_max = select_max\n",
    "        self.allow_skip_mutation = allow_skip_mutation\n",
    "        self.persist_parent = persist_parent\n",
    "        self.parent_params = None\n",
    "        if persist_parent:\n",
    "            # create separate parameter list with shared data\n",
    "            self.parent_params = [p.clone() for p in self.params]\n",
    "            for p, p_parent in zip(self.params, self.parent_params):\n",
    "                p_parent.data = p.data\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        if (\n",
    "            self.active_mutation is not None and\n",
    "            self.active_mutation.reward is None\n",
    "        ):\n",
    "            raise RuntimeError(\"exit before collecting reward for active mutation\")\n",
    "        \n",
    "        # aggregate current mutation set (in case n_mutations\n",
    "        # is not even with number of loop iterations)\n",
    "        if self.mutation_index > -1:\n",
    "            self.aggregate_mutations()\n",
    "\n",
    "    def _param_delta_iter(self, param_seeds: list[int] | None = None):\n",
    "        if param_seeds is None:\n",
    "            param_seeds = [None] * len(self.params)\n",
    "        elif len(param_seeds) != len(self.params):\n",
    "            raise RuntimeError(\"mismatch between number of params and seeds\")\n",
    "        for p, seed in zip(self.params, param_seeds):\n",
    "            # use cpu random number generator for stability\n",
    "            if seed is None:\n",
    "                seed = torch.seed()\n",
    "            else:\n",
    "                torch.manual_seed(seed)\n",
    "            \n",
    "            yield torch.randn(p.shape, dtype=p.dtype).to(p.device) * self.step_size, seed\n",
    "\n",
    "    @property\n",
    "    def mutation_index(self):\n",
    "        return len(self.mutations) - 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def revert_mutation(self):\n",
    "        if not self.active_mutation.is_identity:\n",
    "            if self.persist_parent:\n",
    "                # reset data ptr to current parent (non-mutated) tensor\n",
    "                for p, p_parent in zip(self.params, self.parent_params):\n",
    "                    p.data = p_parent.data\n",
    "            else:\n",
    "                # regenerate deltas from param seeds and subtract to get non-mutated tensor\n",
    "                for p, (delta_p, _) in zip(self.params, self._param_delta_iter(self.active_mutation.param_seeds)):\n",
    "                    p -= delta_p\n",
    "        self.active_mutation = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def mutate(self):\n",
    "        if self.allow_skip_mutation and self.mutation_index == -1:\n",
    "            # save mutation to reference current unperturbed weights\n",
    "            new_mutation = EvMutation(is_identity=True)\n",
    "        else:\n",
    "            # make new mutation\n",
    "            param_seeds = []\n",
    "            for p, (delta_p, seed) in zip(self.params, self._param_delta_iter()):\n",
    "                if self.persist_parent:\n",
    "                    # can't use inplace if we have shared data ptr with current parent\n",
    "                    # instead we create a new tensor from applied delta and update ptr\n",
    "                    # of mutated weight to point to it\n",
    "                    p.data = p + delta_p\n",
    "                else:\n",
    "                    p += delta_p\n",
    "                param_seeds.append(seed)\n",
    "            new_mutation = EvMutation(param_seeds)\n",
    "\n",
    "        self.mutations.append(new_mutation)\n",
    "        self.active_mutation = new_mutation\n",
    "\n",
    "    def reward_step(self, reward: float):\n",
    "        self.active_mutation.reward = reward\n",
    "        self.revert_mutation()\n",
    "\n",
    "        if len(self.mutations) % self.n_mutations == 0:\n",
    "            self.aggregate_mutations()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def aggregate_mutations(self):\n",
    "        if self.active_mutation is not None:\n",
    "            raise RuntimeError(\"cannot aggregate while mutation is still active\")\n",
    "\n",
    "        # print(\"aggregating mutations...\", end=\"\")\n",
    "\n",
    "        if self.select_max:\n",
    "            # get maximum reward candidate\n",
    "            max_r = float('-inf')\n",
    "            max_m = None\n",
    "            for m in self.mutations:\n",
    "                if m.reward > max_r:\n",
    "                    max_r = m.reward\n",
    "                    max_m = m\n",
    "            if not max_m.is_identity:\n",
    "                for p, (p_delta, seed) in zip(self.params, self._param_delta_iter(max_m.param_seeds)):\n",
    "                    # if persist_parent is True, p and p_parent should share data\n",
    "                    # so we need only update one\n",
    "                    p += p_delta\n",
    "\n",
    "        else:       \n",
    "            # weighted avg mutations based on their reward z-score\n",
    "            # all mutations should have reward set by now\n",
    "            n = len(self.mutations)\n",
    "            mean_reward = sum(m.reward for m in self.mutations) / n\n",
    "            var_reward = sum((m.reward - mean_reward) ** 2 for m in self.mutations) / n\n",
    "            z_scores = [(m.reward - mean_reward) / (var_reward ** 0.5) for m in self.mutations if not m.is_identity]\n",
    "    \n",
    "            deltas = [self._param_delta_iter(m.param_seeds) for m in self.mutations if not m.is_identity]\n",
    "            for p in self.params:\n",
    "                p_deltas, _ = zip(*[next(d) for d in deltas])\n",
    "    \n",
    "                for p_delta, z in zip(p_deltas, z_scores):\n",
    "                    p += p_delta * self.lr * z / n\n",
    "\n",
    "        self.mutations = []\n",
    "        # print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1147e241-6f14-4fc5-8097-c3e8ae0d78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_SET_SIZE = 16\n",
    "UPDATES_PER_BATCH = 1\n",
    "# each candidate is evaluated on same batch during each update, UPDATES_PER_BATCH updates per batch\n",
    "ITERS_PER_BATCH = CANDIDATE_SET_SIZE * UPDATES_PER_BATCH\n",
    "LR = 1\n",
    "STEP_SIZE = 0.01 #0.004 #0.001\n",
    "SELECT_MAX = False\n",
    "PERSIST_PARENT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "41e79a14-c4f6-4089-9479-93994fec6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1957df174591463cb35d67c7c3c2a248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== batch 0 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      0     |  8.405111  |  7.371973  |  7.261257\n",
      "==================== batch 1 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      1     |  8.059920  |  7.167271  |  7.021247\n",
      "==================== batch 2 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      2     |  8.150660  |  7.129123  |  7.187538\n",
      "==================== batch 3 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      3     |  8.102978  |  7.098753  |  6.940623\n",
      "==================== batch 4 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      4     |  8.153156  |  7.019162  |  6.890968\n",
      "==================== batch 5 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      5     |  7.833194  |  6.940998  |  6.829600\n",
      "==================== batch 6 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      6     |  7.271399  |  6.614646  |  6.737214\n",
      "==================== batch 7 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      7     |  7.277625  |  6.589547  |  6.652006\n",
      "==================== batch 8 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      8     |  7.175833  |  6.560737  |  6.866717\n",
      "==================== batch 9 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "      9     |  7.449879  |  6.745630  |  6.278561\n",
      "==================== batch 10 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     10     |  7.053559  |  6.475490  |  6.364012\n",
      "==================== batch 11 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     11     |  7.014998  |  6.265694  |  6.178357\n",
      "==================== batch 12 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     12     |  6.725994  |  6.183060  |  6.203194\n",
      "==================== batch 13 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     13     |  6.574494  |  6.116397  |  6.171961\n",
      "==================== batch 14 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     14     |  6.557428  |  6.080542  |  5.920624\n",
      "==================== batch 15 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     15     |  6.372523  |  6.022096  |  6.031774\n",
      "==================== batch 16 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     16     |  6.605247  |  5.796907  |  6.107300\n",
      "==================== batch 17 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     17     |  6.465823  |  5.847039  |  5.983068\n",
      "==================== batch 18 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     18     |  6.334093  |  5.476004  |  5.845881\n",
      "==================== batch 19 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     19     |  6.157252  |  5.296055  |  5.638050\n",
      "==================== batch 20 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     20     |  5.915509  |  5.193405  |  5.683149\n",
      "==================== batch 21 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     21     |  5.856301  |  5.395704  |  5.384256\n",
      "==================== batch 22 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     22     |  5.954650  |  5.571252  |  5.302627\n",
      "==================== batch 23 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     23     |  5.330878  |  4.926483  |  5.329629\n",
      "==================== batch 24 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     24     |  5.149519  |  4.816627  |  5.194148\n",
      "==================== batch 25 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     25     |  5.606484  |  5.295823  |  5.086788\n",
      "==================== batch 26 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     26     |  5.138974  |  4.826061  |  5.098773\n",
      "==================== batch 27 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     27     |  5.176048  |  4.828579  |  5.067209\n",
      "==================== batch 28 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     28     |  5.189581  |  4.494569  |  4.870450\n",
      "==================== batch 29 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     29     |  5.102189  |  4.626158  |  4.444593\n",
      "==================== batch 30 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     30     |  5.140467  |  4.541086  |  4.421982\n",
      "==================== batch 31 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     31     |  4.815295  |  4.380154  |  4.214811\n",
      "==================== batch 32 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     32     |  4.597719  |  4.127912  |  4.467025\n",
      "==================== batch 33 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     33     |  4.551975  |  4.124502  |  4.105910\n",
      "==================== batch 34 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     34     |  4.698829  |  4.199665  |  4.251925\n",
      "==================== batch 35 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     35     |  4.576322  |  4.027519  |  3.989357\n",
      "==================== batch 36 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     36     |  4.923351  |  3.995789  |  3.841236\n",
      "==================== batch 37 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     37     |  4.264282  |  3.678353  |  3.929789\n",
      "==================== batch 38 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     38     |  4.796952  |  3.862098  |  3.686073\n",
      "==================== batch 39 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     39     |  4.140785  |  3.431844  |  3.737621\n",
      "==================== batch 40 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     40     |  5.028122  |  3.796129  |  3.645570\n",
      "==================== batch 41 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     41     |  4.332130  |  3.572375  |  3.593724\n",
      "==================== batch 42 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     42     |  4.453026  |  3.524487  |  3.446068\n",
      "==================== batch 43 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     43     |  4.061982  |  3.289969  |  3.548287\n",
      "==================== batch 44 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     44     |  4.365677  |  3.515063  |  3.487519\n",
      "==================== batch 45 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     45     |  4.082706  |  3.435403  |  3.515890\n",
      "==================== batch 46 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     46     |  3.888527  |  3.455530  |  3.323859\n",
      "==================== batch 47 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     47     |  3.686039  |  3.181596  |  3.676288\n",
      "==================== batch 48 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     48     |  4.262349  |  3.509154  |  3.540001\n",
      "==================== batch 49 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     49     |  3.867401  |  3.425996  |  3.573100\n",
      "==================== batch 50 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     50     |  4.032076  |  3.570769  |  3.298572\n",
      "==================== batch 51 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     51     |  3.681766  |  3.210108  |  3.297510\n",
      "==================== batch 52 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     52     |  3.839050  |  3.291647  |  3.214290\n",
      "==================== batch 53 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     53     |  3.963928  |  3.058762  |  3.120867\n",
      "==================== batch 54 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     54     |  3.838231  |  3.052005  |  3.183615\n",
      "==================== batch 55 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     55     |  4.021775  |  3.210496  |  3.131031\n",
      "==================== batch 56 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     56     |  3.874743  |  3.105742  |  3.097101\n",
      "==================== batch 57 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     57     |  3.820994  |  2.957747  |  2.948169\n",
      "==================== batch 58 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     58     |  4.193153  |  2.939947  |  2.913628\n",
      "==================== batch 59 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...done\n",
      "     59     |  3.790600  |  2.945121  |  3.008797\n",
      "==================== batch 60 ====================\n",
      "    step    |  avg loss  |  min loss  |  val loss\n",
      "aggregating mutations...aggregating mutations...done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     68\u001b[39m                 pbar.update(\u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mtrain_one_epoch_ev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit2_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_val\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-ops/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_one_epoch_ev\u001b[39m\u001b[34m(model, dataloader, val_loader, report_val)\u001b[39m\n\u001b[32m     42\u001b[39m min_loss = loss \u001b[38;5;28;01mif\u001b[39;00m loss < min_loss \u001b[38;5;28;01melse\u001b[39;00m min_loss\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# if final mutation, mutation_index will reset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# report aggregate training loss across all mutations\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# (this will be higher than loss for aggregated model)\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer.mutation_index == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mEvOptimizer.reward_step\u001b[39m\u001b[34m(self, reward)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m.revert_mutation()\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.mutations) % \u001b[38;5;28mself\u001b[39m.n_mutations == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregate_mutations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data-ops/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mEvOptimizer.aggregate_mutations\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m deltas = [\u001b[38;5;28mself\u001b[39m._param_delta_iter(m.param_seeds) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mutations \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m m.is_identity]\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     p_deltas, _ = \u001b[38;5;28mzip\u001b[39m(*[\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m deltas])\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p_delta, z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(p_deltas, z_scores):\n\u001b[32m    150\u001b[39m         p += p_delta * \u001b[38;5;28mself\u001b[39m.lr * z / n\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mEvOptimizer._param_delta_iter\u001b[39m\u001b[34m(self, param_seeds)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     66\u001b[39m     torch.manual_seed(seed)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m.to(p.device) * \u001b[38;5;28mself\u001b[39m.step_size, seed\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def train_one_epoch_ev(model, dataloader, val_loader, report_val=False):\n",
    "    model.eval()  # freeze batchnorm\n",
    "    val_iter = iter(val_loader)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    sample_param = next(model.parameters())\n",
    "    dtype = sample_param.dtype\n",
    "    device = sample_param.device\n",
    "    \n",
    "    with EvOptimizer(model.parameters(),\n",
    "                     step_size=STEP_SIZE,\n",
    "                     lr=LR,\n",
    "                     n_mutations=CANDIDATE_SET_SIZE,\n",
    "                     select_max=SELECT_MAX,\n",
    "                     persist_parent=PERSIST_PARENT) as optimizer:\n",
    "        avg_loss = 0\n",
    "        min_loss = 1000\n",
    "        step_count = 0\n",
    "        pbar = tqdm(total=len(dataloader) * ITERS_PER_BATCH)\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device=device, dtype=dtype), targets.to(device=device)\n",
    "            print(f\"==================== batch {i} ====================\")\n",
    "            print(\"    step    |  avg loss  |  min loss\", end=\"\")\n",
    "            if report_val:\n",
    "                print(\"  |  val loss\")\n",
    "\n",
    "            for j in range(ITERS_PER_BATCH):\n",
    "                optimizer.mutate()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, targets).item()\n",
    "                avg_loss += loss\n",
    "                min_loss = loss if loss < min_loss else min_loss\n",
    "\n",
    "                # if final mutation, mutation_index will reset\n",
    "                optimizer.reward_step(-loss)\n",
    "\n",
    "                \n",
    "                # report aggregate training loss across all mutations\n",
    "                # (this will be higher than loss for aggregated model)\n",
    "                if optimizer.mutation_index == -1:\n",
    "                    avg_loss /= CANDIDATE_SET_SIZE\n",
    "                    step_digits = len(str(step_count))\n",
    "                    l_pad = (12 - step_digits) // 2\n",
    "                    r_pad = 12 - l_pad - step_digits\n",
    "                    print(f\"{' ' * r_pad}{step_count}{' ' * l_pad}|  {avg_loss:1.6f}  |  {min_loss:1.6f}\", end=\"\")\n",
    "\n",
    "                    if report_val:\n",
    "                        val_inputs, val_targets = next(val_iter)\n",
    "                        val_inputs, val_targets = val_inputs.to(device=device, dtype=dtype), val_targets.to(device=device)\n",
    "                        val_outputs = model(val_inputs)\n",
    "                        val_loss = criterion(val_outputs, val_targets).item()\n",
    "                        print(f\"  |  {val_loss:1.6f}\")\n",
    "\n",
    "                    avg_loss = 0\n",
    "                    min_loss = 1000\n",
    "                    step_count += 1\n",
    "\n",
    "                pbar.update(1)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "train_one_epoch_ev(model, split2_dl, val_loader, report_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e8a13785-9ba4-428d-b7f5-321d87e0d9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc370-1ab7-4b95-804e-c4ebefd30d13",
   "metadata": {},
   "source": [
    "### Test equivalence of reverted mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "09d8240d-b29a-4658-a7ea-2a0b74e1cbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1.weight.clone()\n",
    "with torch.no_grad():\n",
    "    seed = torch.seed()\n",
    "    z = model.conv1.weight.to(torch.float64)\n",
    "    z += torch.randn(model.conv1.weight.shape).to(device=0, dtype=torch.float64)\n",
    "    z = z.to(torch.float)\n",
    "    torch.manual_seed(seed)\n",
    "    z = z.to(torch.float64)\n",
    "    z -= torch.randn(model.conv1.weight.shape).to(device=0, dtype=torch.float64)\n",
    "(x == z.to(torch.float)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "70279f9d-821c-470f-8689-d9d4e399d8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1.weight.clone()\n",
    "with torch.no_grad():\n",
    "    seed = torch.seed()\n",
    "    z = model.conv1.weight.to(torch.float64)\n",
    "    z += torch.randn(model.conv1.weight.shape).to(device=0, dtype=torch.float64)\n",
    "    torch.manual_seed(seed)\n",
    "    z -= torch.randn(model.conv1.weight.shape).to(device=0, dtype=torch.float64)\n",
    "(x == z.to(torch.float)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1907fbdf-69dd-417c-82f9-3e1d241d36da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1.weight.clone()\n",
    "with torch.no_grad():\n",
    "    r = torch.randn(model.conv1.weight.shape).to(device=0, dtype=torch.float64)\n",
    "    z = model.conv1.weight.to(torch.float64)\n",
    "    z += r\n",
    "    z -= r\n",
    "(x == z.to(torch.float)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81411ad-c635-47fb-9f98-0800b3e58227",
   "metadata": {},
   "source": [
    "Without upcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "3019ded0-f091-45b1-a245-37818c4fca6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0005, device='cuda:0', dtype=torch.float64, grad_fn=<MinBackward1>),\n",
       " tensor(3.7207, device='cuda:0'))"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.abs().min(), r.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "df286bf8-5d29-4b3d-bbd9-ac08d4b9bdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(False, device='cuda:0'),\n",
       " tensor(8.8818e-16, device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1.weight.clone()\n",
    "with torch.no_grad():\n",
    "    r = torch.randn(model.conv1.weight.shape).to(device=0)\n",
    "    z = model.conv1.weight\n",
    "    z += r\n",
    "    z -= r\n",
    "    # z = -(-z + r)\n",
    "(x == z).all(), (x - z).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cd3c0150-9b40-49a7-87c6-1f4d9ecd1aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.8818e-16, device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(8.8818e-16, device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<MaxBackward1>),\n",
       " tensor(2.2204e-16, device='cuda:0', dtype=torch.float64,\n",
       "        grad_fn=<MaxBackward1>),\n",
       " tensor(8.8818e-16, device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.conv1.weight.clone()\n",
    "with torch.no_grad():\n",
    "    r = torch.randn(model.conv1.weight.shape).to(device=0)\n",
    "    z = model.conv1.weight\n",
    "    z_ = z + r - r\n",
    "    # z_ = z + r\n",
    "    # z_ -= r\n",
    "    diff = z_ - z\n",
    "    print(diff.abs().max())\n",
    "    neg_r = - r - diff\n",
    "    z__ = z + r + neg_r\n",
    "\n",
    "(z - z_).abs().max(), (z - z__).abs().max(), diff.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c764e4-9470-4ca3-8318-19e0feee8ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
